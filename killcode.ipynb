{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from llama_parse import LlamaParse\n",
    "\n",
    "parser = LlamaParse(result_type=\"markdown\", language=\"ko\")\n",
    "\n",
    "file_path = r\"techreader_data\\LLM_TechLibrary.pdf\"\n",
    "parsed_docs = parser.load_data(file_path=file_path)  # ì´ì œ ì •ìƒ ì‹¤í–‰ë¨\n",
    "\n",
    "docs = [doc.to_langchain_format() for doc in parsed_docs]\n",
    "print(docs[0].page_content[:500])  # ì¼ë¶€ ë¯¸ë¦¬ë³´ê¸°\n",
    "\n",
    "import os\n",
    "from llama_parse import LlamaParse\n",
    "\n",
    "# PDF íŒŒì„œ ì´ˆê¸°í™”\n",
    "parser = LlamaParse(\n",
    "    use_vendor_multimodal_model=True,\n",
    "    vendor_multimodal_model_name=\"gemini-2.5-pro\",\n",
    "    vendor_multimodal_api_key=os.environ[\"GOOGLE_API_KEY\"],\n",
    "    result_type=\"markdown\",\n",
    "    parsing_mode=\"Unstructured\",\n",
    "    language=\"ko\",\n",
    "    parsing_instruction=\"\"\"\n",
    "     ë‹¹ì‹ ì€ PDF ë¬¸ì„œë¥¼ êµ¬ì¡°í™”ëœ Markdownìœ¼ë¡œ ë³€í™˜í•˜ëŠ” íŒŒì„œì…ë‹ˆë‹¤.\n",
    "     \n",
    "    ê°€ì¥ ì¤‘ìš”í•œ ê·œì¹™:\n",
    "    ëª¨ë“  í…ìŠ¤íŠ¸ëŠ” ê°€ëŠ¥í•œ ëª¨ë‘ ì¶œë ¥í•´ì£¼ì„¸ìš”. \n",
    "    ì²« í˜ì´ì§€ë¥¼ ì œì™¸í•œ Tech Guideì™€ Tech TrendëŠ” ì†Œì œëª©ì´ ì•„ë‹™ë‹ˆë‹¤. \n",
    "    \n",
    "    \n",
    "    ë³€í™˜ ê·œì¹™:\n",
    "    0. ì›ë¬¸ í…ìŠ¤íŠ¸ëŠ” ê°€ëŠ¥í•œ í•œ ëª¨ë‘ ë³´ì¡´í•˜ì„¸ìš”. \n",
    "    1. ë¬¸ì„œì˜ 'ì£¼ìš” ì œëª©'ì€ ë°˜ë“œì‹œ `# ì œëª©` í˜•ì‹ìœ¼ë¡œ ì¶”ì¶œí•˜ì„¸ìš”.\n",
    "       - ì œëª© ë°”ë¡œ ì•„ë˜ ì¤„ì— 'ì €ì | ì†Œì†'ì´ ìˆìœ¼ë©´ `Author: ì´ë¦„ | ì†Œì†`ìœ¼ë¡œ ì¶œë ¥í•˜ì„¸ìš”.\n",
    "    2. ë³¸ë¬¸ ë‚´ì˜ ì†Œì œëª©ì€ `## ì†Œì œëª©`ìœ¼ë¡œ ë³€í™˜í•˜ì„¸ìš”. \n",
    "       - ë‹¨, '# 1.' ê°™ì€ ë²ˆí˜¸ í˜•ì‹, ì²« í˜ì´ì§€ë¥¼ ì œì™¸í•œ ì˜ì–´ í•œ ë‹¨ì–´, ê²°ë¡ ì„ ì œì™¸í•œ ì§§ì€ ìŒì ˆ(ì˜ˆ: ë¹„ì¶”ì²œ ìš©ë„, ìµœì  ìš©ë„, ì£¼ì˜ì )ì€ ì†Œì œëª©ìœ¼ë¡œ ê°„ì£¼í•˜ì§€ ë§ˆì„¸ìš”.\n",
    "    3. ì œëª©/ì†Œì œëª© ì™¸ì˜ ì¼ë°˜ ë¬¸ë‹¨ì€ ê·¸ëƒ¥ í…ìŠ¤íŠ¸ë¡œ ì¶œë ¥í•˜ì„¸ìš”. íŠ¹ì • í˜ì´ì§€ì— ì¼ë°˜ ë¬¸ë‹¨ë§Œ ìˆì–´ë„ ê·¸ëŒ€ë¡œ ì¶œë ¥í•˜ì„¸ìš”. # í‘œì‹œí•˜ì§€ ë§ˆì„¸ìš”. \n",
    "    4. ì¼ë°˜ ë¬¸ë‹¨ì€ ê·¸ëƒ¥ í…ìŠ¤íŠ¸ë¡œ ì¶œë ¥í•˜ë˜ â€¢ í‘œì‹œë¡œ ì‹œì‘í•˜ëŠ” ê²ƒë„ ê·¸ëŒ€ë¡œ ì¶œë ¥í•´ì£¼ì„¸ìš”.   \n",
    "    5. ëª¨ë“  ì¶œë ¥ì€ ìˆœìˆ˜í•œ Markdown í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”. ë¶ˆí•„ìš”í•œ ì„¤ëª…, ë²ˆì—­, í•´ì„¤ì€ ì ˆëŒ€ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”. í…ìŠ¤íŠ¸ë¥¼ ìš”ì•½í•˜ì§€ ë§ê³  ê·¸ëŒ€ë¡œ ì¶œë ¥í•˜ì„¸ìš”. \n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# íŒŒì¼ ê²½ë¡œ\n",
    "file_path = r\"techreader_data\\LLM_TechLibrary.pdf\"\n",
    "# TechReader_gayoon\\techreader_data\\LLM_TechLibrary.pdf\n",
    "# PDF â†’ íŒŒì‹±\n",
    "parsed_docs = parser.load_data(file_path=file_path)\n",
    "\n",
    "# LangChain Document ë³€í™˜\n",
    "docs = [doc.to_langchain_format() for doc in parsed_docs]\n",
    "\n",
    "# Markdown ì €ì¥\n",
    "file_root, _ = os.path.splitext(file_path)\n",
    "output_file_path = file_root + \"_parsed0903_gemini.md\"\n",
    "\n",
    "full_text = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "with open(output_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(full_text)\n",
    "\n",
    "print(f\"âœ… íŒŒì¼ ì €ì¥ ì™„ë£Œ: {output_file_path}\")\n",
    "\n",
    "\n",
    "import os\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "\n",
    "# 1. ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "with open(\"techreader_data/LLM_TechLibrary_parsed0903_gemini.md\", \"r\", encoding=\"utf-8\") as f:\n",
    "    markdown_text = f.read()\n",
    "\n",
    "# 2. ë¶„í•  ê¸°ì¤€ í—¤ë” ì •ì˜\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),    # # ì œëª©\n",
    "    (\"##\", \"Header 2\"),   # ## ì†Œì œëª©\n",
    "    (\"###\", \"Header 3\"),  # ### í•˜ìœ„ ì†Œì œëª©\n",
    "]\n",
    "\n",
    "# 3. Splitter ì´ˆê¸°í™”\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "\n",
    "# 4. ë¬¸ì„œ ë¶„í• \n",
    "md_header_splits = markdown_splitter.split_text(markdown_text)\n",
    "\n",
    "# 5. ê²°ê³¼ ì¶œë ¥\n",
    "for i, doc in enumerate(md_header_splits[:10]):  # ì•ì—ì„œ 10ê°œë§Œ ì¶œë ¥\n",
    "    print(f\"==== Chunk {i+1} ====\")\n",
    "    print(doc.page_content[:300])  # ì•ë¶€ë¶„ ë¯¸ë¦¬ë³´ê¸°\n",
    "    print(\"ë©”íƒ€ë°ì´í„°:\", doc.metadata)\n",
    "    print()\n",
    "\n",
    "import csv\n",
    "\n",
    "with open(\"techreader_data/chunks_output.csv\", \"w\", newline=\"\", encoding=\"utf-8-sig\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Chunk No\", \"Metadata\", \"Content\"])\n",
    "    for i, doc in enumerate(md_header_splits, start=1):\n",
    "        writer.writerow([i, doc.metadata, doc.page_content.strip()])\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "import os\n",
    "\n",
    "# 1. OpenAI Embedding ì´ˆê¸°í™”\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "# 2. md_header_splits (ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸)ë¥¼ ë²¡í„°í™” í›„ ì €ì¥\n",
    "vectorstore = FAISS.from_documents(md_header_splits, embeddings)\n",
    "\n",
    "# 3. ì €ì¥\n",
    "vectorstore.save_local(\"faiss_index\")\n",
    "\n",
    "print(\"âœ… ë²¡í„°ìŠ¤í† ì–´ ì €ì¥ ì™„ë£Œ: faiss_index í´ë” ìƒì„±ë¨\")\n",
    "\n",
    "\n",
    "# ì €ì¥ëœ FAISS ì¸ë±ìŠ¤ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "new_vectorstore = FAISS.load_local(\"faiss_index\", embeddings, allow_dangerous_deserialization=True)\n",
    "\n",
    "# ê²€ìƒ‰ \n",
    "query = \"êµì‚¬ ëª¨ë¸ê³¼ í•™ìƒ ëª¨ë¸ì˜ ê´€ê³„ì—ì„œ ë°œìƒí•˜ëŠ” ë³´ì•ˆ ìœ„í—˜ì€ ë¬´ì—‡ì¸ê°€ìš”?\"\n",
    "docs = new_vectorstore.similarity_search(query, k=3)\n",
    "\n",
    "for d in docs:\n",
    "    print(\"ğŸ“Œ\", d.page_content[:300])\n",
    "    print(\"ë©”íƒ€ë°ì´í„°:\", d.metadata)\n",
    "    print(\"=\"*50)\n",
    "\n",
    "\n",
    "import os\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Google AI Studioì—ì„œ ë°œê¸‰ë°›ì€ API í‚¤ ì„¤ì •\n",
    "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
    "\n",
    "# Gemini ëª¨ë¸ ì´ˆê¸°í™” (ìµœì‹ ì€ gemini-1.5-pro ë˜ëŠ” gemini-1.5-flash)\n",
    "model = genai.GenerativeModel(\"gemini-2.5-pro\")\n",
    "\n",
    "def generate_questions(header1, header2=None):\n",
    "    topic = header1 if header2 is None else f\"{header1} - {header2}\"\n",
    "    prompt = f\"\"\"\n",
    "    ë„ˆëŠ” AI ìµœì‹  ê¸°ìˆ  ë™í–¥ì„ ë¶„ì„í•´ ì‚¬ë‚´ ì—”ì§€ë‹ˆì–´ì™€ ì—°êµ¬ì›ì´ ë¹ ë¥´ê²Œ ì´í•´í•  ìˆ˜ ìˆë„ë¡\n",
    "    í•µì‹¬ ìŸì ì„ ì§ˆë¬¸ í˜•íƒœë¡œ ì •ë¦¬í•˜ëŠ” ì—­í• ì´ë‹¤.\n",
    "    ì§€ê¸ˆ ë‹¤ë£¨ëŠ” ë¬¸ì„œëŠ” Tech Library Top1ì— ì„ ì •ëœ 21í˜ì´ì§€ì§œë¦¬ ë¦¬í¬íŠ¸ì´ë©°, ì¬ì§ì ì „ìš©ì´ë‹¤.\n",
    "    ë„ˆì˜ ëª©í‘œëŠ” ì£¼ì œì™€ ê´€ë ¨ëœ ê¸°ìˆ ì  ìŸì ê³¼ ì—°êµ¬ ë°©í–¥ì„ ë“œëŸ¬ë‚´ëŠ” ì˜ˆìƒ ì§ˆë¬¸ì„ ìƒì„±í•˜ëŠ” ê²ƒì´ë‹¤.\n",
    "\n",
    "    [ìš”êµ¬ì‚¬í•­]\n",
    "    - ì£¼ì œ: {topic}\n",
    "    - ì˜ˆìƒ ì§ˆë¬¸ì€ ì´ 5ê°œë¥¼ ë§Œë“¤ì–´ë¼.\n",
    "    - ì§ˆë¬¸ì€ í•™ìƒìš© ë‹¨ìˆœ ì´í•´ ì°¨ì›ì´ ì•„ë‹ˆë¼, ì—”ì§€ë‹ˆì–´/ì—°êµ¬ìê°€\n",
    "      í† ë¡ Â·ì‹¤í—˜Â·ì„¤ê³„ ë‹¨ê³„ì—ì„œ ì‹¤ì œë¡œ ê³ ë¯¼í•  ë§Œí•œ 'ê¸°ìˆ ì  ì§ˆë¬¸'ì´ì–´ì•¼ í•œë‹¤.\n",
    "    - ì§ˆë¬¸ì€ ëª…í™•í•˜ê³  êµ¬ì²´ì ìœ¼ë¡œ ì‘ì„±í•˜ë¼.\n",
    "    \"\"\"\n",
    "\n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text.strip().split(\"\\n\")\n",
    "\n",
    "\n",
    "questions_dict = {}\n",
    "\n",
    "for doc in md_header_splits:\n",
    "    h1 = doc.metadata.get(\"Header 1\")\n",
    "    h2 = doc.metadata.get(\"Header 2\")\n",
    "\n",
    "    if (h1, h2) not in questions_dict:\n",
    "        q_list = generate_questions(h1, h2)\n",
    "        questions_dict[(h1, h2)] = q_list\n",
    "\n",
    "# í™•ì¸\n",
    "for (h1, h2), q_list in questions_dict.items():\n",
    "    print(f\"\\nğŸ“Œ {h1} > {h2 if h2 else ''}\")\n",
    "    for q in q_list:\n",
    "        print(\" -\", q)\n",
    "\n",
    "\n",
    "# ë³´ê³ ì„œ ë³¸ë¬¸ ê¸°ë°˜ ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸\n",
    "content_questions_dict = {}\n",
    "\n",
    "for doc in md_header_splits:\n",
    "    h1 = doc.metadata.get(\"Header 1\")\n",
    "    h2 = doc.metadata.get(\"Header 2\")\n",
    "    content = doc.page_content\n",
    "\n",
    "    if (h1, h2) not in content_questions_dict:\n",
    "        q_list = generate_questions_from_content(h1, h2, content)\n",
    "        content_questions_dict[(h1, h2)] = q_list\n",
    "\n",
    "# í™•ì¸\n",
    "for (h1, h2), q_list in content_questions_dict.items():\n",
    "    print(f\"\\nğŸ“Œ {h1} > {h2 if h2 else ''}\")\n",
    "    for q in q_list:\n",
    "        print(\" -\", q)\n",
    "\n",
    "\n",
    "# ë‹µë³€ë„ ê°™ì´ ìƒì„±í•˜ê¸° \n",
    "import csv\n",
    "\n",
    "def load_questions(csv_path):\n",
    "    questions = []\n",
    "    with open(csv_path, \"r\", encoding=\"utf-8-sig\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            questions.append({\n",
    "                \"Header 1\": row[\"Header 1\"],\n",
    "                \"Header 2\": row[\"Header 2\"],\n",
    "                \"Question\": row[\"Question\"]\n",
    "            })\n",
    "    return questions\n",
    "\n",
    "questions = load_questions(\"techreader_data/content_based_questions_clean.csv\")\n",
    "print(f\"ì´ {len(questions)}ê°œ ì§ˆë¬¸ ë¶ˆëŸ¬ì˜´\")\n",
    "\n",
    "import google.generativeai as genai\n",
    "import os, re\n",
    "\n",
    "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
    "model = genai.GenerativeModel(\"gemini-2.5-pro\")\n",
    "\n",
    "def clean_answer_text(text: str) -> str:\n",
    "    # ë¶ˆí•„ìš”í•œ ë©˜íŠ¸ ì œê±°\n",
    "    text = re.sub(r\"(ë„¤, ì•Œê² ìŠµë‹ˆë‹¤.*|ë¬¼ë¡ ì…ë‹ˆë‹¤.*|ë‹¤ìŒì€.*|ì•„ë˜ì™€ ê°™ì´.*)\", \"\", text)\n",
    "    text = re.sub(r\"[*#]{2,}\", \"\", text)  # ###, *** ì œê±°\n",
    "    # ë¬¸ì¥ ëì˜ ... â†’ .\n",
    "    text = re.sub(r\"\\.{2,}\", \".\", text.strip())\n",
    "    return text.strip()\n",
    "\n",
    "def generate_answer(question, header1, header2, content):\n",
    "    prompt = f\"\"\"\n",
    "    ë„ˆëŠ” AI ìµœì‹  ê¸°ìˆ  ë¦¬í¬íŠ¸ë¥¼ ë¶„ì„í•˜ëŠ” LLM ì—”ì§€ë‹ˆì–´ë‹¤.\n",
    "    ë¬¸ì„œ ì£¼ì œ: {header1} - {header2 if header2 else \"\"}\n",
    "    ë³¸ë¬¸ ë‚´ìš© (ë°œì·Œ): {content[:3000] if content else \"\"}\n",
    "\n",
    "    [ìš”êµ¬ì‚¬í•­]\n",
    "    - ì•„ë˜ ì§ˆë¬¸ì— ëŒ€í•´ ë³´ê³ ì„œ ë³¸ë¬¸ì„ ê·¼ê±°ë¡œ ì‹¬ì¸µì ì¸ ë‹µë³€ì„ ì‘ì„±í•˜ë¼.\n",
    "    - ë‹µë³€ì€ ì—°êµ¬ ë³´ê³ ì„œ ìŠ¤íƒ€ì¼ë¡œ ì‘ì„±í•˜ë©°, 3~4ë¬¸ë‹¨ìœ¼ë¡œ êµ¬ì„±í•˜ë¼.\n",
    "    - ì „ì²´ ë¶„ëŸ‰ì€ 800~1000ì ë‚´ì™¸ê°€ ë˜ë„ë¡ í•˜ë¼.\n",
    "    - ê° ë¬¸ë‹¨ì€ ì™„ê²°ëœ ë¬¸ì¥ìœ¼ë¡œ ëë‚´ë¼.\n",
    "    - ì„œë¡ (ì§ˆë¬¸ì˜ ì¤‘ìš”ì„±), ë³¸ë¡ (ê¸°ìˆ ì  ê·¼ê±°Â·ì„¸ë¶€ ë¶„ì„), ê²°ë¡ (í•µì‹¬ ìš”ì•½ê³¼ ì‹œì‚¬ì ) êµ¬ì¡°ë¥¼ ë”°ë¥´ë¼.\n",
    "    - ë§ˆì§€ë§‰ ë¬¸ì¥ì€ ë°˜ë“œì‹œ ë§ˆì¹¨í‘œ í•˜ë‚˜(.)ë¡œ ëë‚´ë¼. ë¶ˆí•„ìš”í•œ ... ì€ ì“°ì§€ ë§ë¼.\n",
    "    - ì¶œë ¥ì€ ë°˜ë“œì‹œ 'ë‹µë³€: ' í˜•ì‹ìœ¼ë¡œ í•˜ë¼.\n",
    "    - ìµœì¢… ë‹µë³€ì€ ë°˜ë“œì‹œ ë§ˆì¹¨í‘œ í•˜ë‚˜(.)ë¡œ ëë‚´ë¼.\n",
    "    - '...' ë‚˜ ë¶ˆí•„ìš”í•œ ë°˜ë³µ ë§ˆì¹¨í‘œëŠ” ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ë¼.\n",
    "\n",
    "\n",
    "    ì§ˆë¬¸: {question}\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        response = model.generate_content(\n",
    "            prompt,\n",
    "            generation_config={\"max_output_tokens\": 9096, \"temperature\": 0.7}\n",
    "        )\n",
    "\n",
    "        if response.candidates and response.candidates[0].content.parts:\n",
    "            answer = response.candidates[0].content.parts[0].text.strip()\n",
    "        else:\n",
    "            answer = \"[âš ï¸ ë‹µë³€ ì—†ìŒ: í† í° í•œë„ ì´ˆê³¼ ë˜ëŠ” ì•ˆì „ í•„í„° ì°¨ë‹¨]\"\n",
    "\n",
    "    except Exception as e:\n",
    "        answer = f\"[âš ï¸ ì—ëŸ¬ ë°œìƒ: {str(e)}]\"\n",
    "\n",
    "    # í›„ì²˜ë¦¬: ê²°ë¡  ë³´ê°•\n",
    "    if not answer.endswith((\"ì´ë‹¤.\", \"ìˆë‹¤.\", \"í•  ìˆ˜ ìˆë‹¤.\")):\n",
    "        try:\n",
    "            fix_prompt = f\"\"\"\n",
    "            ë‹¤ìŒ ë‹µë³€ì´ ê²°ë¡  ì—†ì´ ëë‚¬ìŠµë‹ˆë‹¤. \n",
    "            ë¶ˆí•„ìš”í•œ ë©˜íŠ¸(ì˜ˆ: 'ë„¤, ì•Œê² ìŠµë‹ˆë‹¤', 'ë¬¼ë¡ ì…ë‹ˆë‹¤', 'ë‹¤ìŒì€', '### ê²°ë¡ ', '***')ëŠ” ì“°ì§€ ë§ê³ , \n",
    "            ë³´ê³ ì„œ ìŠ¤íƒ€ì¼ì˜ ê²°ë¡  ë¬¸ë‹¨(3~4ë¬¸ì¥)ì„ ì‘ì„±í•˜ì„¸ìš”. ë§ˆì§€ë§‰ ë¬¸ì¥ì€ ë°˜ë“œì‹œ ë§ˆì¹¨í‘œ í•˜ë‚˜(.)ë¡œ ëë‚´ë¼.\n",
    "\n",
    "            ë¶ˆì™„ì „ ë‹µë³€: {answer}\n",
    "            \"\"\"\n",
    "            fix_response = model.generate_content(fix_prompt)\n",
    "            if fix_response.candidates and fix_response.candidates[0].content.parts:\n",
    "                answer += \"\\n\\n\" + fix_response.candidates[0].content.parts[0].text.strip()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # ë§ˆì§€ë§‰ ì •ë¦¬ (ë©˜íŠ¸/###/*** ì œê±°, ... â†’ .)\n",
    "    return clean_answer_text(answer) or \"\"\n",
    "\n",
    "for i, q in enumerate(questions):  # ì „ì²´ ë‹¤ ëŒë¦¼\n",
    "    h1, h2, question = q[\"Header 1\"], q[\"Header 2\"], q[\"Question\"]\n",
    "\n",
    "    # md_header_splitsì—ì„œ í—¤ë” ë§¤ì¹­í•´ì„œ ë³¸ë¬¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "    content = \"\"\n",
    "    for doc in md_header_splits:\n",
    "        if doc.metadata.get(\"Header 1\") == h1 and doc.metadata.get(\"Header 2\") == h2:\n",
    "            content = doc.page_content\n",
    "            break\n",
    "\n",
    "    answer = generate_answer(question, h1, h2, content)\n",
    "    q[\"Answer\"] = answer  # ë‹µë³€ ì¶”ê°€\n",
    "\n",
    "    print(f\"\\nQ{i+1}/{len(questions)}: {question}\")\n",
    "    print(f\"A: {answer[:]}...\")  # ì•ë¶€ë¶„ë§Œ ë¯¸ë¦¬ë³´ê¸° \n",
    "    \n",
    "    \n",
    "    \n",
    "import csv\n",
    "\n",
    "output_path = \"techreader_data/content_based_questions_with_answers.csv\"\n",
    "\n",
    "with open(output_path, \"w\", newline=\"\", encoding=\"utf-8-sig\") as f:\n",
    "    fieldnames = [\"Header 1\", \"Header 2\", \"Question\", \"Answer\"]\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "\n",
    "    for q in questions:\n",
    "        writer.writerow({\n",
    "            \"Header 1\": q[\"Header 1\"],\n",
    "            \"Header 2\": q[\"Header 2\"],\n",
    "            \"Question\": q[\"Question\"],\n",
    "            \"Answer\": q.get(\"Answer\", \"\")\n",
    "        })\n",
    "\n",
    "print(f\"âœ… ìµœì¢… ì €ì¥ ì™„ë£Œ: {output_path}\")\n",
    "\n",
    "\n",
    "import csv\n",
    "\n",
    "def load_questions(csv_path):\n",
    "    questions = []\n",
    "    with open(csv_path, \"r\", encoding=\"utf-8-sig\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            questions.append({\n",
    "                \"Header 1\": row[\"Header 1\"],\n",
    "                \"Header 2\": row[\"Header 2\"],\n",
    "                \"Question\": row[\"Question\"]\n",
    "            })\n",
    "    return questions\n",
    "\n",
    "questions = load_questions(\"techreader_data/header_based_questions_clean.csv\")\n",
    "print(f\"ì´ {len(questions)}ê°œ ì§ˆë¬¸ ë¶ˆëŸ¬ì˜´\")\n",
    "\n",
    "import google.generativeai as genai\n",
    "import os, re\n",
    "\n",
    "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
    "model = genai.GenerativeModel(\"gemini-2.5-pro\")\n",
    "\n",
    "def clean_answer_text(text: str) -> str:\n",
    "    # ë¶ˆí•„ìš”í•œ ë©˜íŠ¸ ì œê±°\n",
    "    text = re.sub(r\"(ë„¤, ì•Œê² ìŠµë‹ˆë‹¤.*|ë¬¼ë¡ ì…ë‹ˆë‹¤.*|ë‹¤ìŒì€.*|ì•„ë˜ì™€ ê°™ì´.*)\", \"\", text)\n",
    "    text = re.sub(r\"[*#]{2,}\", \"\", text)  # ###, *** ì œê±°\n",
    "    # ë¬¸ì¥ ëì˜ ... â†’ .\n",
    "    text = re.sub(r\"\\.{2,}\", \".\", text.strip())\n",
    "    return text.strip()\n",
    "\n",
    "def generate_answer(question, header1, header2, content):\n",
    "    prompt = f\"\"\"\n",
    "    ë„ˆëŠ” AI ìµœì‹  ê¸°ìˆ  ë¦¬í¬íŠ¸ë¥¼ ë¶„ì„í•˜ëŠ” LLM ì—”ì§€ë‹ˆì–´ë‹¤.\n",
    "    ë¬¸ì„œ ì£¼ì œ: {header1} - {header2 if header2 else \"\"}\n",
    "    ë³¸ë¬¸ ë‚´ìš© (ë°œì·Œ): {content[:3000] if content else \"\"}\n",
    "\n",
    "    [ìš”êµ¬ì‚¬í•­]\n",
    "    - ì•„ë˜ ì§ˆë¬¸ì— ëŒ€í•´ ë³´ê³ ì„œ ë³¸ë¬¸ì„ ê·¼ê±°ë¡œ ì‹¬ì¸µì ì¸ ë‹µë³€ì„ ì‘ì„±í•˜ë¼.\n",
    "    - ë‹µë³€ì€ ì—°êµ¬ ë³´ê³ ì„œ ìŠ¤íƒ€ì¼ë¡œ ì‘ì„±í•˜ë©°, 3~4ë¬¸ë‹¨ìœ¼ë¡œ êµ¬ì„±í•˜ë¼.\n",
    "    - ì „ì²´ ë¶„ëŸ‰ì€ 800~1000ì ë‚´ì™¸ê°€ ë˜ë„ë¡ í•˜ë¼.\n",
    "    - ê° ë¬¸ë‹¨ì€ ì™„ê²°ëœ ë¬¸ì¥ìœ¼ë¡œ ëë‚´ë¼.\n",
    "    - ì„œë¡ (ì§ˆë¬¸ì˜ ì¤‘ìš”ì„±), ë³¸ë¡ (ê¸°ìˆ ì  ê·¼ê±°Â·ì„¸ë¶€ ë¶„ì„), ê²°ë¡ (í•µì‹¬ ìš”ì•½ê³¼ ì‹œì‚¬ì ) êµ¬ì¡°ë¥¼ ë”°ë¥´ë¼.\n",
    "    - ë§ˆì§€ë§‰ ë¬¸ì¥ì€ ë°˜ë“œì‹œ ë§ˆì¹¨í‘œ í•˜ë‚˜(.)ë¡œ ëë‚´ë¼. ë¶ˆí•„ìš”í•œ ... ì€ ì“°ì§€ ë§ë¼.\n",
    "    - ì¶œë ¥ì€ ë°˜ë“œì‹œ 'ë‹µë³€: ' í˜•ì‹ìœ¼ë¡œ í•˜ë¼.\n",
    "    - ìµœì¢… ë‹µë³€ì€ ë°˜ë“œì‹œ ë§ˆì¹¨í‘œ í•˜ë‚˜(.)ë¡œ ëë‚´ë¼.\n",
    "    - '...' ë‚˜ ë¶ˆí•„ìš”í•œ ë°˜ë³µ ë§ˆì¹¨í‘œëŠ” ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ë¼.\n",
    "\n",
    "\n",
    "    ì§ˆë¬¸: {question}\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        response = model.generate_content(\n",
    "            prompt,\n",
    "            generation_config={\"max_output_tokens\": 9096, \"temperature\": 0.7}\n",
    "        )\n",
    "\n",
    "        if response.candidates and response.candidates[0].content.parts:\n",
    "            answer = response.candidates[0].content.parts[0].text.strip()\n",
    "        else:\n",
    "            answer = \"[âš ï¸ ë‹µë³€ ì—†ìŒ: í† í° í•œë„ ì´ˆê³¼ ë˜ëŠ” ì•ˆì „ í•„í„° ì°¨ë‹¨]\"\n",
    "\n",
    "    except Exception as e:\n",
    "        answer = f\"[âš ï¸ ì—ëŸ¬ ë°œìƒ: {str(e)}]\"\n",
    "\n",
    "    # í›„ì²˜ë¦¬: ê²°ë¡  ë³´ê°•\n",
    "    if not answer.endswith((\"ì´ë‹¤.\", \"ìˆë‹¤.\", \"í•  ìˆ˜ ìˆë‹¤.\")):\n",
    "        try:\n",
    "            fix_prompt = f\"\"\"\n",
    "            ë‹¤ìŒ ë‹µë³€ì´ ê²°ë¡  ì—†ì´ ëë‚¬ìŠµë‹ˆë‹¤. \n",
    "            ë¶ˆí•„ìš”í•œ ë©˜íŠ¸(ì˜ˆ: 'ë„¤, ì•Œê² ìŠµë‹ˆë‹¤', 'ë¬¼ë¡ ì…ë‹ˆë‹¤', 'ë‹¤ìŒì€', '### ê²°ë¡ ', '***')ëŠ” ì“°ì§€ ë§ê³ , \n",
    "            ë³´ê³ ì„œ ìŠ¤íƒ€ì¼ì˜ ê²°ë¡  ë¬¸ë‹¨(3~4ë¬¸ì¥)ì„ ì‘ì„±í•˜ì„¸ìš”. ë§ˆì§€ë§‰ ë¬¸ì¥ì€ ë°˜ë“œì‹œ ë§ˆì¹¨í‘œ í•˜ë‚˜(.)ë¡œ ëë‚´ë¼.\n",
    "\n",
    "            ë¶ˆì™„ì „ ë‹µë³€: {answer}\n",
    "            \"\"\"\n",
    "            fix_response = model.generate_content(fix_prompt)\n",
    "            if fix_response.candidates and fix_response.candidates[0].content.parts:\n",
    "                answer += \"\\n\\n\" + fix_response.candidates[0].content.parts[0].text.strip()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # ë§ˆì§€ë§‰ ì •ë¦¬ (ë©˜íŠ¸/###/*** ì œê±°, ... â†’ .)\n",
    "    return clean_answer_text(answer) or \"\"\n",
    "\n",
    "for i, q in enumerate(questions):  # ì „ì²´ ë‹¤ ëŒë¦¼\n",
    "    h1, h2, question = q[\"Header 1\"], q[\"Header 2\"], q[\"Question\"]\n",
    "\n",
    "    # md_header_splitsì—ì„œ í—¤ë” ë§¤ì¹­í•´ì„œ ë³¸ë¬¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "    content = \"\"\n",
    "    for doc in md_header_splits:\n",
    "        if doc.metadata.get(\"Header 1\") == h1 and doc.metadata.get(\"Header 2\") == h2:\n",
    "            content = doc.page_content\n",
    "            break\n",
    "\n",
    "    answer = generate_answer(question, h1, h2, content)\n",
    "    q[\"Answer\"] = answer  # ë‹µë³€ ì¶”ê°€\n",
    "\n",
    "    print(f\"\\nQ{i+1}/{len(questions)}: {question}\")\n",
    "    print(f\"A: {answer[:]}...\")  # ì•ë¶€ë¶„ë§Œ ë¯¸ë¦¬ë³´ê¸° \n",
    "    \n",
    "    \n",
    "    \n",
    "import csv\n",
    "\n",
    "output_path = \"techreader_data/header_based_questions_with_answers.csv\"\n",
    "\n",
    "with open(output_path, \"w\", newline=\"\", encoding=\"utf-8-sig\") as f:\n",
    "    fieldnames = [\"Header 1\", \"Header 2\", \"Question\", \"Answer\"]\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "\n",
    "    for q in questions:\n",
    "        writer.writerow({\n",
    "            \"Header 1\": q[\"Header 1\"],\n",
    "            \"Header 2\": q[\"Header 2\"],\n",
    "            \"Question\": q[\"Question\"],\n",
    "            \"Answer\": q.get(\"Answer\", \"\")\n",
    "        })\n",
    "\n",
    "print(f\"âœ… ìµœì¢… ì €ì¥ ì™„ë£Œ: {output_path}\")\n",
    "\n",
    "# ì˜ˆìƒ ì§ˆë¬¸ ë‹µë³€ ìŒ ì €ì¥ ì™„ë£Œ (CSV íŒŒì¼ 2ê°œ) \n",
    "\n",
    "\n",
    "# ìœ„ì˜ paraphraseëœ ì§ˆë¬¸ì—ëŠ” ì§ˆë¬¸ì„ í¬í•¨í•œ ê´‘ë²”ìœ„í•œ ë‚´ìš©ì´ ë‹´ê²¨ í•„í„°ë§ í•„ìš” - ìš°ì„  ì‹œë²” í…ŒìŠ¤íŠ¸ ì§„í–‰ \n",
    "import pandas as pd\n",
    "import os\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Gemini ì´ˆê¸°í™”\n",
    "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
    "model = genai.GenerativeModel(\"gemini-2.5-pro\")\n",
    "\n",
    "def generate_paraphrases(question, n=5):\n",
    "    prompt = f\"\"\"\n",
    "    ë‹¤ìŒ ì§ˆë¬¸ì„ ì˜ë¯¸ê°€ ë™ì¼í•˜ì§€ë§Œ í‘œí˜„ì´ ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ {n}ê°œ ë§Œë“¤ì–´ì¤˜.\n",
    "    ì¶œë ¥ì€ ë°˜ë“œì‹œ ì§ˆë¬¸ë§Œ, ê° ì¤„ í•˜ë‚˜ì”©, ë¶ˆí•„ìš”í•œ ì„¤ëª…ì´ë‚˜ ë²ˆí˜¸, ë¶ˆë¦¿, ë§ˆí¬ë‹¤ìš´ ê¸°í˜¸ ì—†ì´ ì‘ì„±í•´.\n",
    "    ì§ˆë¬¸: {question}\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = model.generate_content(prompt)\n",
    "        if response.candidates and response.candidates[0].content.parts:\n",
    "            text = response.candidates[0].content.parts[0].text.strip()\n",
    "            # ì¤„ ë‹¨ìœ„ split\n",
    "            lines = [line.strip(\" -â€¢0123456789.\") for line in text.split(\"\\n\") if line.strip()]\n",
    "            # \"?\" ë¡œ ëë‚˜ëŠ” ì§ˆë¬¸ë§Œ ë‚¨ê¹€\n",
    "            paras = [line for line in lines if line.endswith(\"?\")]\n",
    "            return paras\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "    return []\n",
    "\n",
    "# ì›ë³¸ CSV ë¡œë“œ\n",
    "input_path = \"techreader_data/content_based_questions_with_answers.csv\"\n",
    "df = pd.read_csv(input_path)\n",
    "\n",
    "# ìƒìœ„ 3ê°œ ì§ˆë¬¸ë§Œ í…ŒìŠ¤íŠ¸\n",
    "sample_questions = df[\"Question\"].head(3).tolist()\n",
    "\n",
    "print(\"ğŸ”¹ Paraphrase ìƒì„± í…ŒìŠ¤íŠ¸ (3ê°œ ì§ˆë¬¸)\\n\")\n",
    "for i, q in enumerate(sample_questions, start=1):\n",
    "    paras = generate_paraphrases(q, n=3)\n",
    "    print(f\"Q{i}: {q}\")\n",
    "    for j, p in enumerate(paras, start=1):\n",
    "        print(f\"   â†’ P{j}: {p}\") \n",
    "    print(\"-\" * 60)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Gemini ì´ˆê¸°í™”\n",
    "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
    "model = genai.GenerativeModel(\"gemini-2.5-pro\")\n",
    "\n",
    "def generate_paraphrases(question, n=4):\n",
    "    prompt = f\"\"\"\n",
    "    ë‹¤ìŒ ì§ˆë¬¸ì„ ì˜ë¯¸ê°€ ë™ì¼í•˜ì§€ë§Œ í‘œí˜„ì´ ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ {n}ê°œ ë§Œë“¤ì–´ì¤˜.\n",
    "    ì¶œë ¥ì€ ë°˜ë“œì‹œ ì§ˆë¬¸ë§Œ, ê° ì¤„ í•˜ë‚˜ì”©, ë¶ˆí•„ìš”í•œ ì„¤ëª…ì´ë‚˜ ë²ˆí˜¸, ë¶ˆë¦¿, ë§ˆí¬ë‹¤ìš´ ê¸°í˜¸ ì—†ì´ ì‘ì„±í•´.\n",
    "    ì§ˆë¬¸: {question}\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = model.generate_content(prompt)\n",
    "        if response.candidates and response.candidates[0].content.parts:\n",
    "            text = response.candidates[0].content.parts[0].text.strip()\n",
    "            lines = [line.strip(\" -â€¢0123456789.\") for line in text.split(\"\\n\") if line.strip()]\n",
    "            paras = [line for line in lines if line.endswith(\"?\")]  # ì§ˆë¬¸ë§Œ\n",
    "            return paras\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "    return []\n",
    "\n",
    "# ì›ë³¸ CSV ë¡œë“œ\n",
    "input_path = \"techreader_data/content_based_questions_with_answers.csv\"\n",
    "df = pd.read_csv(input_path)\n",
    "\n",
    "# Paraphrases ìƒì„±\n",
    "df[\"Paraphrases\"] = df[\"Question\"].apply(lambda q: generate_paraphrases(q, n=4))\n",
    "\n",
    "# ìƒˆ íŒŒì¼ë¡œ ì €ì¥\n",
    "output_path = \"techreader_data/content_based_FAQ2_with_paraphrases.csv\"\n",
    "df.to_csv(output_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"âœ… Paraphrases ì¶”ê°€ ì™„ë£Œ: {output_path}\")\n",
    "\n",
    "# TechReader_gayoon/techreader_data/header_based_questions_with_answers.csv \n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Gemini ì´ˆê¸°í™”\n",
    "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
    "model = genai.GenerativeModel(\"gemini-2.5-pro\")\n",
    "\n",
    "def generate_paraphrases(question, n=4):\n",
    "    prompt = f\"\"\"\n",
    "    ë‹¤ìŒ ì§ˆë¬¸ì„ ì˜ë¯¸ê°€ ë™ì¼í•˜ì§€ë§Œ í‘œí˜„ì´ ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ {n}ê°œ ë§Œë“¤ì–´ì¤˜.\n",
    "    ì¶œë ¥ì€ ë°˜ë“œì‹œ ì§ˆë¬¸ë§Œ, ê° ì¤„ í•˜ë‚˜ì”©, ë¶ˆí•„ìš”í•œ ì„¤ëª…ì´ë‚˜ ë²ˆí˜¸, ë¶ˆë¦¿, ë§ˆí¬ë‹¤ìš´ ê¸°í˜¸ ì—†ì´ ì‘ì„±í•´.\n",
    "    ì§ˆë¬¸: {question}\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = model.generate_content(prompt)\n",
    "        if response.candidates and response.candidates[0].content.parts:\n",
    "            text = response.candidates[0].content.parts[0].text.strip()\n",
    "            lines = [line.strip(\" -â€¢0123456789.\") for line in text.split(\"\\n\") if line.strip()]\n",
    "            paras = [line for line in lines if line.endswith(\"?\")]  # ì§ˆë¬¸ë§Œ\n",
    "            return paras\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "    return []\n",
    "\n",
    "# ì›ë³¸ CSV ë¡œë“œ\n",
    "input_path = \"techreader_data/header_based_questions_with_answers.csv \"\n",
    "df = pd.read_csv(input_path)\n",
    "\n",
    "# Paraphrases ìƒì„±\n",
    "df[\"Paraphrases\"] = df[\"Question\"].apply(lambda q: generate_paraphrases(q, n=4))\n",
    "\n",
    "# ìƒˆ íŒŒì¼ë¡œ ì €ì¥\n",
    "output_path = \"techreader_data/header_based_FAQ2_with_paraphrases.csv\"\n",
    "df.to_csv(output_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"âœ… Paraphrases ì¶”ê°€ ì™„ë£Œ: {output_path}\")\n",
    "\n",
    "\n",
    "faq ì§ˆë¬¸ ë‹µë³€ìŒì€ CacheBackedEmbeddings ì‚¬ìš© \n",
    "ì¼ë°˜ ì§ˆë¬¸ ë‹µë³€ìŒì€ ì—…ìŠ¤í…Œì´ì§€ ì„ë² ë”© ëª¨ë¸ ì‚¬ìš© \n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import ast\n",
    "from langchain.schema import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "from langchain_upstage import UpstageEmbeddings\n",
    "from langchain.storage import LocalFileStore\n",
    "from pathlib import Path\n",
    "\n",
    "embeddings = UpstageEmbeddings(model=\"solar-embedding-1-large\", api_key=\"UPSTAGE_API_KEY\")\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 1. FAQ ë°ì´í„° (CacheBackedEmbeddings)\n",
    "# ----------------------------\n",
    "def load_faq_docs(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    docs = []\n",
    "    for _, row in df.iterrows():\n",
    "        base_q = row[\"Question\"]\n",
    "        paras = []\n",
    "        try:\n",
    "            paras = ast.literal_eval(row[\"Paraphrases\"])\n",
    "        except:\n",
    "            pass\n",
    "        all_qs = [base_q] + paras\n",
    "        for q in all_qs:\n",
    "            docs.append(Document(\n",
    "                page_content=q,\n",
    "                metadata={\"Answer\": row[\"Answer\"],\n",
    "                          \"Header 1\": row.get(\"Header 1\", \"\"),\n",
    "                          \"Header 2\": row.get(\"Header 2\", \"\")}\n",
    "            ))\n",
    "    return docs\n",
    "\n",
    "faq_files = [\n",
    "    \"techreader_data/header_based_FAQ2_with_paraphrases.csv\",\n",
    "    \"techreader_data/content_based_FAQ2_with_paraphrases.csv\"\n",
    "]\n",
    "\n",
    "faq_docs = []\n",
    "for f in faq_files:\n",
    "    faq_docs.extend(load_faq_docs(f))\n",
    "\n",
    "# ìºì‹œ ë””ë ‰í† ë¦¬\n",
    "cache_dir = Path(\"techreader_data/faq_cache\")\n",
    "store = LocalFileStore(str(cache_dir))\n",
    "\n",
    "# ê¸°ë³¸ ì„ë² ë”©\n",
    "base_embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# CacheBackedEmbeddings êµ¬ì„±\n",
    "faq_embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
    "    underlying_embeddings=base_embeddings,\n",
    "    document_embedding_cache=store\n",
    ")\n",
    "\n",
    "faq_db = FAISS.from_documents(faq_docs, faq_embeddings)\n",
    "faq_db.save_local(\"techreader_data/faq_index\")\n",
    "faq_retriever = faq_db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "\n",
    "print(\"âœ… FAQ Retriever ì¤€ë¹„ ì™„ë£Œ\")\n",
    "\n",
    "import pandas as pd\n",
    "import ast\n",
    "from langchain.schema import Document\n",
    "\n",
    "chunks_df = pd.read_csv(\"techreader_data/chunks_output.csv\")\n",
    "\n",
    "chunk_docs = []\n",
    "for _, row in chunks_df.iterrows():\n",
    "    content = row[\"Content\"]\n",
    "\n",
    "    # Metadata ë¬¸ìì—´ â†’ dict ë³€í™˜\n",
    "    metadata = {}\n",
    "    if isinstance(row[\"Metadata\"], str) and row[\"Metadata\"].strip() != \"{}\":\n",
    "        try:\n",
    "            metadata = ast.literal_eval(row[\"Metadata\"])\n",
    "        except Exception:\n",
    "            metadata = {\"raw_metadata\": row[\"Metadata\"]}\n",
    "\n",
    "    # Chunk ë²ˆí˜¸ë„ ì¶”ê°€\n",
    "    metadata[\"Chunk No\"] = row[\"Chunk No\"]\n",
    "\n",
    "    chunk_docs.append(Document(page_content=content, metadata=metadata))\n",
    "\n",
    "print(f\"âœ… ì´ {len(chunk_docs)}ê°œ chunk ë³€í™˜ ì™„ë£Œ\")\n",
    "print(\"ì˜ˆì‹œ Document:\", chunk_docs[0])\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_upstage import UpstageEmbeddings  # pip install langchain-upstage\n",
    "\n",
    "# Upstage ì„ë² ë”© ì´ˆê¸°í™”\n",
    "chunk_embeddings = UpstageEmbeddings(\n",
    "    model=\"solar-embedding-1-large\", \n",
    "    api_key=os.environ[\"UPSTAGE_API_KEY\"]\n",
    ")\n",
    "\n",
    "# FAISS ì¸ë±ìŠ¤ êµ¬ì¶•\n",
    "chunk_db = FAISS.from_documents(chunk_docs, chunk_embeddings)\n",
    "\n",
    "# ë¡œì»¬ ì €ì¥\n",
    "chunk_db.save_local(\"techreader_data/chunk_index\")\n",
    "\n",
    "# Retriever\n",
    "chunk_retriever = chunk_db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "\n",
    "print(\"âœ… Chunk Retriever ì¤€ë¹„ ì™„ë£Œ\")\n",
    "\n",
    "\n",
    "query = \"êµì‚¬ì™€ í•™ìƒ ëª¨ë¸ì˜ ì°¨ì´ì ì€ ë¬´ì—‡ì¸ê°€?\"\n",
    "\n",
    "response = hybrid_search(query, faq_retriever, chunk_retriever)\n",
    "print(response)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
